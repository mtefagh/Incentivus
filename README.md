# Welcome to _Project Incentivus_!

**Project Incentivus** is a new research initiative aiming to explore convex optimization applications to the incentive layer of blockchain protocols.

This interdisciplinary effort started from an earlier [work](https://ethresear.ch/t/draft-position-paper-on-resource-pricing/2838/24?u=mtefagh) on [EIP 1559](https://ethereum-magicians.org/t/eip-1559-fee-market-change-for-eth-1-0-chain/2783/24?u=mtefagh). In summary, this EIP suggests an alternative to the current first-price auction transaction fee market. In this scheme, all of the transactions in a block share a universal fee; however, this fee is dynamically updated across different blocks according to a predetermined in-protocol formula. We started to look at a question relevant to all users: _what is the optimal policy to pay the minimum fee for executing a set of transactions?_ Such problems have long been investigated in economics and are frequently solved by convex optimization. In this case, the optimal policy turned out to be too optimal and could be considered an [attack](https://nbviewer.jupyter.org/github/mtefagh/fee/blob/master/fee.ipynb) in the sense that controlling the allocation of a small fraction of the total volume of transactions is enough to manipulate the price and make it zero in a short time.

Moreover, instead of searching for the optimal behavior given a specific protocol design, we can turn such arguments around to engineer user behaviors by setting the right incentives. A simple observation is that the transaction fee pricing problem is similar in many aspects to the liquidation strategies in economics. In the former case, we want to execute many transactions, but each time we do so, the price of executing the next transaction goes up. In the latter case, we want to sell our assets, but each time we do so, the trading price goes down. An established model for the second problem is the [Almgrenâ€“Chriss](https://github.com/zcash/zcash/issues/3473#issuecomment-479625462) framework, which guarantees that the optimal execution of transaction strategy is to spread the transactions across time which in turn helps to avoid network congestion for blockchains. Overall, we believe that although such use cases of convex optimization are exploited extensively in game theory, they have not transitioned to the blockchain ecosystem yet.

## Transaction fee pricing as a special case of matching logic

A simple observation is that the problem of specifying the right _equilibrium price_ (EP) for transaction fees is a particular instance of the more general problem of determining the EP in a DEX match engine. In this analogy, a single seller, which is the group of validators, is selling the block size amount of space to a large number of buyers, which are the user transactions. A continuous match engine results in the first-price auction and a discrete one gives the second-price auction for the fee market.

In order to show this, consider the steps in [this article](https://docs.binance.org/match-examples.html). Step 0  corresponds to the case of mining an empty block. Step 1 corresponds to choosing a price which fills up the block. Step 2 corresponds to choosing the EP in a way that if the transactions associated to the **n** highest bids fill up the block, then EP must be chosen between the **n** and **n+1** bids. If we assume that the number of bids is large enough such that the **n** and **n+1** bids are close enough, this already gives the second-price auction, and the steps 3 and 4 are irrelevant.

### The same problem in both cases

The Binance DEX has given numerous [warnings](https://docs.binance.org/match.html#conclude-execution-price) regarding the aggressive orders in very volatile or illiquid markets. The problem is that it might be the case that one can even profit from selling less amount. This problem is precisely the same as when in second-price auctions a miner may profit from forging fake transactions in order to shrink the available block size. In both cases, the significant change in the final price makes up for the lesser quantity that one sells. 

To show this problem by an example, consider an order book with one sell bid for 20 units at price 5, one buy bid for 10 units at price 11, and another buy bid for 10 units at price 5. According to the [Binance DEX logic](https://www.binance.vision/tutorials/deep-dive-into-the-binance-dex-match-engine), in this scenario the seller earns 100. However, if the last buy bid did not exist, the seller would earn 110 while retaining half of his assets. Another way to see this point is that selling only 10 units instead of 20 units is in seller's favor because the money he earns is not an increasing function of the amount he sells.

Note that, this problem exists in any periodic auction with a single universal price in each period, where EP is set to be the price at which the maximum volume can be traded. In other words, to solve this problem, one should either match at multiple different prices like in the first-price auction or waive maximizing the execution quantity like in a recent proposal to [redesign Bitcoin's fee market](https://arxiv.org/abs/1709.08881). In that article, the authors propose a monopolistic price mechanism in which the miners choose the optimal block size for maximizing their profit. This mechanism corresponds to the seller in our example, choosing to sell only ten units, which is the analog of reducing the block size. 

## Clearing frequency and market thickness

The incentive incompatibility discussed in the previous section can be solved by increasing the market thickness. As a result, clearing the not-so-liquid markets at a lower frequency solves this problem at the expense of delay. The tradeoff between immediacy and batching in frequent call markets has been [studied](http://simonloertscher.net/wp-content/uploads/2018/04/LMT.2018-02-12.pdf) for a long time. It is well-known that higher frequencies reduce market liquidity and allocative efficiency and also suffer from the adverse effects of high-frequency traders exploiting latency arbitrage. We refer the interested reader to [here](https://dl.acm.org/citation.cfm?id=3085153) for more details.

On the other hand, blockchains also have a limited block size, which can be either fixed or adaptive. Dividing this block size between the different trading pairs is a crucial design choice if the sum of the required spaces exceeds the block size. Together with the clearing frequency, both the clearing interval and volume which are a multiple of block time and a fraction of block size are adjustable. Note that, by volume, in here we mean the number of trades (or bytes to be more accurate) instead of the more conventional trading volume. 

Depending on the relative adoption and scalability, clearing all the markets at every single block will eventually become infeasible for any DEX. Here we propose that in such inevitable situations, the available time and space should be divided proportionately to the liquidity of each market. Furthermore, this limitation of blockchains increases the efficiency of illiquid markets compared to the continuous matching or more frequent repeated auctions as suggested by many [studies](https://academic.oup.com/restud/article/84/4/1606/2963139). Also, it increases the transparency of how the validators handpick trades from the mempool by reducing the degree of freedom in this framework compared to the arbitrary selection or fee arms race. 

### Liveness of DEX

The problem of allocating block space as a resource to different trading pairs is comparable to [scheduling](https://en.wikipedia.org/wiki/Scheduling_(computing)) in many respects. One of these aspects is the [liveness](https://en.wikipedia.org/wiki/Liveness) property which can be defined in analogy to computer science by stating that a match engine provides a weak or strong liveness guarantee if any market is guaranteed to be cleared in a finite or bounded number of blocks respectively. As a result, these properties provide upper bounds on the wait time a user may experience. We can apply any of the [priority inversion](https://en.wikipedia.org/wiki/Priority_inversion) methods such as random boosting to achieve such guarantees.
